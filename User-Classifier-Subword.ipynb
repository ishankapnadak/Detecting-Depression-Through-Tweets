{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1607874690761,
     "user": {
      "displayName": "Thomas Jacob",
      "photoUrl": "",
      "userId": "15901388336454847226"
     },
     "user_tz": -330
    },
    "id": "v6YpLEc_r4wM"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2873,
     "status": "ok",
     "timestamp": 1607874695991,
     "user": {
      "displayName": "Thomas Jacob",
      "photoUrl": "",
      "userId": "15901388336454847226"
     },
     "user_tz": -330
    },
    "id": "bbCZ6j3qtcN5"
   },
   "outputs": [],
   "source": [
    "import data_helpers \n",
    "import re \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from data_helpers import BPE\n",
    "\n",
    "# Data Preparation 1: HTML decoding\n",
    "pat1 = r'@[A-Za-z0-9]+'   # @mention\n",
    "pat2 = r'https?://[A-Za-z0-9./]+' # URL\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text, 'lxml')  # HTML decoding\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped) # URL, @mention\n",
    "    \n",
    "    try:\n",
    "        clean = stripped.replace(u'ï¿½', ' ') # Latin character encoding\n",
    "    except:\n",
    "        clean = stripped\n",
    "    \n",
    "    lower_case = clean.lower()\n",
    "    return lower_case\n",
    "\n",
    "def filter_tweet(row):\n",
    "\n",
    "    tweet = tweet_cleaner(row['tweet'])\n",
    "    return tweet\n",
    "\n",
    "# Convert subword to index, function version\n",
    "def subword2index(texts, vocab):\n",
    "    \n",
    "    sentences = []\n",
    "    for s in texts:\n",
    "    \n",
    "        s = s.split()\n",
    "        one_line = []\n",
    "        \n",
    "        for word in s:\n",
    "            \n",
    "            if word not in vocab.keys():\n",
    "                one_line.append(vocab['unk'])\n",
    "            else:\n",
    "                one_line.append(vocab[word])\n",
    "        \n",
    "        sentences.append(one_line)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def subword_rep(tweets):\n",
    "\n",
    "    texts = [re.sub('\\d', '0', s) for s in tweets]\n",
    "\n",
    "    # replace all URLs with <url>\n",
    "    url_reg = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "    texts = [re.sub(url_reg, '<url>', s) for s in texts]\n",
    "\n",
    "    # Convert string to subword, this process may take several minutes\n",
    "    bpe = BPE(\"en.wiki.bpe.op25000.vocab\")\n",
    "    texts = [bpe.encode(s) for s in texts]\n",
    "\n",
    "    # Build vocab, {token: index}\n",
    "    vocab = {}\n",
    "    for i, token in enumerate(bpe.words):\n",
    "        vocab[token] = i + 1\n",
    "\n",
    "    # Convert texts\n",
    "    sentences = subword2index(texts, vocab)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1412,
     "status": "ok",
     "timestamp": 1607874719691,
     "user": {
      "displayName": "Thomas Jacob",
      "photoUrl": "",
      "userId": "15901388336454847226"
     },
     "user_tz": -330
    },
    "id": "ddJi94SBsFai"
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import load_model\n",
    "cnn_model = load_model('cnn-sub.h5')\n",
    "gru_model = load_model('gru-sub.h5')\n",
    "lstm_cnn_model = load_model('lstm-cnn-sub.h5')\n",
    "\n",
    "def classify_cnn(username):\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    c = twint.Config()\n",
    "    c.Username = username\n",
    "    c.Hide_output = True\n",
    "    c.Limit = 500\n",
    "    c.Pandas = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = twint.storage.panda.Tweets_df\n",
    "    \n",
    "    if (len(df)<10):\n",
    "        print(\"Insufficient Data\")\n",
    "        return \n",
    "\n",
    "    else:\n",
    "        df = df[['tweet']]\n",
    "        df['tweet'] = df.apply(filter_tweet, axis=1)\n",
    "\n",
    "        tweets = df['tweet'].values\n",
    "        sentences = subword_rep(tweets)\n",
    "        data = pad_sequences(sentences, maxlen=252, padding='post')\n",
    "\n",
    "        labels_pred = cnn_model.predict(data)\n",
    "        labels_pred = np.round(labels_pred)\n",
    "        labels_pred = np.argmax(labels_pred, axis=1)\n",
    "        \n",
    "        total = len(df)\n",
    "        positive = labels_pred.sum()\n",
    "        ratio = positive/total\n",
    "        \n",
    "        print(f\"Number of tweets obtained : {total}\")\n",
    "        print(f\"Fraction of tweets that are indicative of depression : {ratio}\")\n",
    "        \n",
    "        if (ratio<=0.02):\n",
    "            print(\"At low to no risk of depression\")\n",
    "            return\n",
    "        elif (ratio<=0.1):\n",
    "            print(\"At mild risk of depression\")\n",
    "            return\n",
    "        elif (ratio<=0.35):\n",
    "            print(\"At moderate risk of depression\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"At significant risk of depression\")\n",
    "            return\n",
    "           \n",
    "def classify_gru(username):\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    c = twint.Config()\n",
    "    c.Username = username\n",
    "    c.Hide_output = True\n",
    "    c.Limit = 500\n",
    "    c.Pandas = True\n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    df = twint.storage.panda.Tweets_df\n",
    "    \n",
    "    if (len(df)<10):\n",
    "        print(\"Insufficient Data\")\n",
    "        return \n",
    "\n",
    "    else:\n",
    "        df = df[['tweet']]\n",
    "        df['tweet'] = df.apply(filter_tweet, axis=1)\n",
    "\n",
    "        tweets = df['tweet'].values\n",
    "        sentences = subword_rep(tweets)\n",
    "        data = pad_sequences(sentences, maxlen=252, padding='post')\n",
    "\n",
    "        labels_pred = gru_model.predict(data)\n",
    "        labels_pred = np.round(labels_pred)\n",
    "        labels_pred = np.argmax(labels_pred, axis=1)\n",
    "        \n",
    "        total = len(df)\n",
    "        positive = labels_pred.sum()\n",
    "        ratio = positive/total\n",
    "        \n",
    "        print(f\"Number of tweets obtained : {total}\")\n",
    "        print(f\"Fraction of tweets that are indicative of depression : {ratio}\")\n",
    "        \n",
    "        if (ratio<=0.02):\n",
    "            print(\"At low to no risk of depression\")\n",
    "            return\n",
    "        elif (ratio<=0.1):\n",
    "            print(\"At mild risk of depression\")\n",
    "            return\n",
    "        elif (ratio<=0.35):\n",
    "            print(\"At moderate risk of depression\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"At significant risk of depression\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Celebrities with Depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9wvjtvkzQXo",
    "outputId": "f2f214f8-d6b2-47b5-8444-b0f621a85678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.154\n",
      "At moderate risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.152\n",
      "At moderate risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"TheRock\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G1KRwm5szdtt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.128\n",
      "At moderate risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.178\n",
      "At moderate risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"katyperry\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.078\n",
      "At mild risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.12\n",
      "At moderate risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"ladygaga\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.096\n",
      "At mild risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.106\n",
      "At moderate risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"jk_rowling\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.106\n",
      "At moderate risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.03\n",
      "At mild risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"deepikapadukone\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Twitter Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.012\n",
      "At low to no risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.002\n",
      "At low to no risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"ndtv\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.006\n",
      "At low to no risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.002\n",
      "At low to no risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"bonappetit\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.006\n",
      "At low to no risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.022\n",
      "At mild risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"IPL\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.006\n",
      "At low to no risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.044\n",
      "At mild risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"AnimalPlanet\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.016\n",
      "At low to no risk of depression\n",
      "\n",
      "Number of tweets obtained : 500\n",
      "Fraction of tweets that are indicative of depression : 0.02\n",
      "At low to no risk of depression\n"
     ]
    }
   ],
   "source": [
    "username = \"Funfacts\"\n",
    "classify_cnn(username)\n",
    "print()\n",
    "classify_gru(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPQFQRdl0n78CEj1ndPZg9u",
   "name": "User-Classifier-Subword.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
